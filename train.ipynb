{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install the necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U medpy==0.4.0 scipy==1.10.1 scikit-image==0.19.3 numpy==1.20.0 tensorboard==2.14.0 setuptools==59.5.0 tqdm\n",
    "\n",
    "# If you're using MindSpore, install the CPU version of pytorch (for tensorboard):\n",
    "# %pip install torch torchvision \n",
    "\n",
    "# If you're using pytorch, follow the official website's installation guide instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import packages and define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T15:56:14.141179Z",
     "start_time": "2020-06-11T15:56:05.371997Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import os.path as osp\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from dataset import BrainSegmentationDataset as Dataset\n",
    "from logger import Logger\n",
    "from loss import DiceLoss\n",
    "from transform import transforms\n",
    "from unet import UNet\n",
    "from utils import log_images, dsc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T15:56:14.185956Z",
     "start_time": "2020-06-11T15:56:14.151605Z"
    }
   },
   "outputs": [],
   "source": [
    "def worker_init(worker_id):\n",
    "    np.random.seed(42 + worker_id)\n",
    "\n",
    "def data_loaders(args):\n",
    "    dataset_train, dataset_valid = datasets(args)\n",
    "\n",
    "    loader_train = DataLoader(\n",
    "        dataset_train,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        num_workers=args.workers,\n",
    "        pin_memory=False,  # no GPU\n",
    "        worker_init_fn=worker_init,  # Use the standalone function\n",
    "    )\n",
    "    loader_valid = DataLoader(\n",
    "        dataset_valid,\n",
    "        batch_size=args.batch_size,\n",
    "        drop_last=False,\n",
    "        num_workers=args.workers,\n",
    "        pin_memory=False,  # no GPU\n",
    "        worker_init_fn=worker_init,  # Use the standalone function\n",
    "    )\n",
    "\n",
    "    return loader_train, loader_valid\n",
    "\n",
    "\n",
    "def datasets(args):\n",
    "    train = Dataset(\n",
    "        images_dir=args.images,\n",
    "        subset=\"train\",\n",
    "        image_size=args.image_size,\n",
    "        transform=transforms(scale=args.aug_scale, angle=args.aug_angle, flip_prob=0.5),\n",
    "    )\n",
    "    valid = Dataset(\n",
    "        images_dir=args.images,\n",
    "        subset=\"validation\",\n",
    "        image_size=args.image_size,\n",
    "        random_sampling=False,\n",
    "    )\n",
    "    return train, valid\n",
    "\n",
    "\n",
    "def dsc_per_volume(validation_pred, validation_true, patient_slice_index):\n",
    "    dsc_list = []\n",
    "    num_slices = np.bincount([p[0] for p in patient_slice_index])\n",
    "    index = 0\n",
    "    for p in range(len(num_slices)):\n",
    "        y_pred = np.array(validation_pred[index : index + num_slices[p]])\n",
    "        y_true = np.array(validation_true[index : index + num_slices[p]])\n",
    "        dsc_list.append(dsc(y_pred, y_true))\n",
    "        index += num_slices[p]\n",
    "    return dsc_list\n",
    "\n",
    "\n",
    "def log_loss_summary(logger, loss, step, prefix=\"\"):\n",
    "    logger.scalar_summary(prefix + \"loss\", np.mean(loss), step)\n",
    "\n",
    "\n",
    "def makedirs(args):\n",
    "    os.makedirs(args.weights, exist_ok=True)\n",
    "    os.makedirs(args.logs, exist_ok=True)\n",
    "\n",
    "\n",
    "def snapshotargs(args):\n",
    "    args_file = os.path.join(args.logs, \"args.json\")\n",
    "    with open(args_file, \"w\") as fp:\n",
    "        json.dump(vars(args), fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set arguments  \n",
    "  \n",
    "device: device    \n",
    "batch_size: batch size     \n",
    "epochs: number of training epochs    \n",
    "lr: learning rate   \n",
    "vis_images: number of images for visualization (in tensorboard)     \n",
    "vis_freq: interval between two visualizations      \n",
    "weights: path to save trained model weights     \n",
    "images: path to the dataset     \n",
    "image_size: size of the input image        \n",
    "aug_scale: data augmentation (rescale)   \n",
    "aug_angle: data augmentation (rotation)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T16:42:43.314801Z",
     "start_time": "2020-06-11T15:56:14.267240Z"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Args:\n",
    "    device = 'cuda:0'\n",
    "    batch_size = 16\n",
    "    epochs = 5\n",
    "    lr = 0.0003\n",
    "    workers = 4\n",
    "    vis_images = 200\n",
    "    vis_freq = 10\n",
    "    weights = './weights'\n",
    "    logs = './logs'\n",
    "    images = 'BrainMRI/kaggle_3m'\n",
    "    image_size = 256\n",
    "    aug_scale = 0.05\n",
    "    aug_angle = 15\n",
    "\n",
    "args = Args()\n",
    "\n",
    "# Tips: Training duration is about 1~3 hours with about 20 min image preprocessing time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert osp.exists(args.images), \"Please download the dataset and set the correct path\" \n",
    "\n",
    "# save configs\n",
    "makedirs(args)\n",
    "snapshotargs(args)\n",
    "\n",
    "device = torch.device(\"cpu\" if not torch.cuda.is_available() else args.device)\n",
    "\n",
    "# build dataset\n",
    "loader_train, loader_valid = data_loaders(args)\n",
    "loaders = {\"train\": loader_train, \"valid\": loader_valid}\n",
    "\n",
    "# build model\n",
    "unet = UNet(in_channels=Dataset.in_channels, out_channels=Dataset.out_channels)\n",
    "unet.to(device)\n",
    "\n",
    "checkpoint_path = os.path.join(args.weights, \"unet.pt\")\n",
    "if os.path.exists(checkpoint_path):\n",
    "    unet.load_state_dict(torch.load(checkpoint_path))\n",
    "    print(\"Loaded checkpoint from\", checkpoint_path)\n",
    "\n",
    "# build optimizer\n",
    "optimizer = optim.Adam(unet.parameters(), lr=args.lr)\n",
    "\n",
    "# build metric\n",
    "dsc_loss = DiceLoss()\n",
    "\n",
    "# build loggers (use tensorboard to visualize the loss curves)\n",
    "best_validation_dsc = 0.0\n",
    "\n",
    "logger = Logger(args.logs)\n",
    "loss_train = []\n",
    "loss_valid = []\n",
    "\n",
    "step = 0\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    for phase in [\"train\", \"valid\"]:\n",
    "        print(\"epoch {}, phase {}, total step {}\".format(epoch, phase, step))\n",
    "\n",
    "        if phase == \"train\":\n",
    "            unet.train()\n",
    "        else:\n",
    "            unet.eval()\n",
    "\n",
    "        validation_pred = []\n",
    "        validation_true = []\n",
    "\n",
    "        for i, data in enumerate(loaders[phase]):\n",
    "            if phase == \"train\":\n",
    "                step += 1\n",
    "\n",
    "            x, y_true = data\n",
    "            x, y_true = x.to(device), y_true.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(phase == \"train\"):\n",
    "                y_pred = unet(x)\n",
    "\n",
    "                loss = dsc_loss(y_pred, y_true)\n",
    "\n",
    "                if phase == \"valid\":\n",
    "                    loss_valid.append(loss.item())\n",
    "                    y_pred_np = y_pred.detach().cpu().numpy()\n",
    "                    validation_pred.extend(\n",
    "                        [y_pred_np[s] for s in range(y_pred_np.shape[0])]\n",
    "                    )\n",
    "                    y_true_np = y_true.detach().cpu().numpy()\n",
    "                    validation_true.extend(\n",
    "                        [y_true_np[s] for s in range(y_true_np.shape[0])]\n",
    "                    )\n",
    "                    if (epoch % args.vis_freq == 0) or (epoch == args.epochs - 1):\n",
    "                        if i * args.batch_size < args.vis_images:\n",
    "                            tag = \"image/{}\".format(i)\n",
    "                            num_images = args.vis_images - i * args.batch_size\n",
    "                            logger.image_list_summary(\n",
    "                                tag,\n",
    "                                log_images(x, y_true, y_pred)[:num_images],\n",
    "                                step,\n",
    "                            )\n",
    "\n",
    "                if phase == \"train\":\n",
    "                    loss_train.append(loss.item())\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            if phase == \"train\" and (step + 1) % 10 == 0:\n",
    "                log_loss_summary(logger, loss_train, step)\n",
    "                print(\"epoch {}, phase {}, step {}, loss_train, {:.3f}\".format(\n",
    "                        epoch, phase, step, np.mean(loss_train)))\n",
    "                loss_train = []\n",
    "\n",
    "        if phase == \"valid\":\n",
    "            log_loss_summary(logger, loss_valid, step, prefix=\"val_\")\n",
    "            print(\"epoch {} | val_loss: {}\".format(epoch + 1, np.mean(loss_valid)))\n",
    "            mean_dsc = np.mean(\n",
    "                dsc_per_volume(\n",
    "                    validation_pred,\n",
    "                    validation_true,\n",
    "                    loader_valid.dataset.patient_slice_index,\n",
    "                )\n",
    "            )\n",
    "            logger.scalar_summary(\"val_dsc\", mean_dsc, step)\n",
    "            print(\"epoch {} | val_dsc: {}\".format(epoch+1, mean_dsc))\n",
    "            if mean_dsc > best_validation_dsc:\n",
    "                best_validation_dsc = mean_dsc\n",
    "                torch.save(unet.state_dict(), os.path.join(args.weights, \"unet.pt\"))\n",
    "            loss_valid = []\n",
    "\n",
    "print(\"Best validation mean DSC: {:4f}\".format(best_validation_dsc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
